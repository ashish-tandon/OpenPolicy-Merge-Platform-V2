"""
ETL Scheduler for OpenParliament.ca V2
Following FUNDAMENTAL RULE: Scheduling all legacy data ingestion jobs
"""
import asyncio
import logging
import schedule
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable
from pathlib import Path
import sys

# Add app to path
sys.path.append(str(Path(__file__).parent.parent))

from app.ingestion.multi_level_government_ingester import MultiLevelGovernmentIngester
from app.ingestion.legacy_data_ingester import LegacyDataIngester
from app.ingestion.municipal_data_ingester import MunicipalDataIngester
from app.config import get_database_url, get_etl_config

logger = logging.getLogger(__name__)

class ETLScheduler:
    """Comprehensive ETL scheduler for all data ingestion jobs"""
    
    def __init__(self):
        self.database_url = get_database_url()
        self.config = get_etl_config()
        self.running_jobs: Dict[str, bool] = {}
        self.job_history: List[Dict] = []
        self.scheduler = schedule.Scheduler()
        
    async def initialize(self):
        """Initialize the ETL scheduler"""
        logger.info("üöÄ Initializing ETL Scheduler")
        
        # Test database connection
        try:
            async with MultiLevelGovernmentIngester(self.database_url) as ingester:
                await ingester.initialize_database()
            logger.info("‚úÖ Database connection successful")
        except Exception as e:
            logger.error(f"‚ùå Database connection failed: {str(e)}")
            raise
    
    def schedule_all_jobs(self):
        """Schedule all ETL jobs"""
        logger.info("üìÖ Scheduling all ETL jobs")
        
        # Daily jobs (02:00 UTC)
        self.scheduler.every().day.at("02:00").do(
            lambda: asyncio.create_task(self._run_daily_jobs())
        )
        
        # Weekly jobs (Sunday 03:00 UTC)
        self.scheduler.every().sunday.at("03:00").do(
            lambda: asyncio.create_task(self._run_weekly_jobs())
        )
        
        # Bi-weekly jobs (Tuesday 04:00 UTC)
        self.scheduler.every().tuesday.at("04:00").do(
            lambda: asyncio.create_task(self._run_biweekly_jobs())
        )
        
        # Monthly jobs (1st of month 05:00 UTC)
        self.scheduler.every().month.at("05:00").do(
            lambda: asyncio.create_task(self._run_monthly_jobs())
        )
        
        logger.info("‚úÖ All ETL jobs scheduled")
    
    async def _run_daily_jobs(self):
        """Run daily ETL jobs"""
        job_name = "daily_etl_jobs"
        if self.running_jobs.get(job_name):
            logger.warning(f"‚è≥ Daily jobs already running, skipping")
            return
        
        self.running_jobs[job_name] = True
        start_time = datetime.now()
        
        try:
            logger.info("üåÖ Starting daily ETL jobs")
            
            # 1. Federal representatives update
            await self._update_federal_representatives()
            
            # 2. OpenParliament data sync
            await self._sync_openparliament_data()
            
            # 3. API health checks
            await self._check_api_health()
            
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "success", duration)
            logger.info(f"‚úÖ Daily ETL jobs completed in {duration.total_seconds():.2f} seconds")
            
        except Exception as e:
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "failed", duration, str(e))
            logger.error(f"‚ùå Daily ETL jobs failed: {str(e)}")
        finally:
            self.running_jobs[job_name] = False
    
    async def _run_weekly_jobs(self):
        """Run weekly ETL jobs"""
        job_name = "weekly_etl_jobs"
        if self.running_jobs.get(job_name):
            logger.warning(f"‚è≥ Weekly jobs already running, skipping")
            return
        
        self.running_jobs[job_name] = True
        start_time = datetime.now()
        
        try:
            logger.info("üìÖ Starting weekly ETL jobs")
            
            # 1. Municipal data refresh
            await self._refresh_municipal_data()
            
            # 2. CSV scraper execution
            await self._run_csv_scrapers()
            
            # 3. Data quality validation
            await self._validate_data_quality()
            
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "success", duration)
            logger.info(f"‚úÖ Weekly ETL jobs completed in {duration.total_seconds():.2f} seconds")
            
        except Exception as e:
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "failed", duration, str(e))
            logger.error(f"‚ùå Weekly ETL jobs failed: {str(e)}")
        finally:
            self.running_jobs[job_name] = False
    
    async def _run_biweekly_jobs(self):
        """Run bi-weekly ETL jobs"""
        job_name = "biweekly_etl_jobs"
        if self.running_jobs.get(job_name):
            logger.warning(f"‚è≥ Bi-weekly jobs already running, skipping")
            return
        
        self.running_jobs[job_name] = True
        start_time = datetime.now()
        
        try:
            logger.info("üîÑ Starting bi-weekly ETL jobs")
            
            # 1. Provincial data update
            await self._update_provincial_data()
            
            # 2. Legacy scraper execution
            await self._run_legacy_scrapers()
            
            # 3. Schema validation
            await self._validate_schema()
            
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "success", duration)
            logger.info(f"‚úÖ Bi-weekly ETL jobs completed in {duration.total_seconds():.2f} seconds")
            
        except Exception as e:
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "failed", duration, str(e))
            logger.error(f"‚ùå Bi-weekly ETL jobs failed: {str(e)}")
        finally:
            self.running_jobs[job_name] = False
    
    async def _run_monthly_jobs(self):
        """Run monthly ETL jobs"""
        job_name = "monthly_etl_jobs"
        if self.running_jobs.get(job_name):
            logger.warning(f"‚è≥ Monthly jobs already running, skipping")
            return
        
        self.running_jobs[job_name] = True
        start_time = datetime.now()
        
        try:
            logger.info("üìä Starting monthly ETL jobs")
            
            # 1. Full legacy scraper run
            await self._run_full_legacy_scrapers()
            
            # 2. Data archival
            await self._archive_old_data()
            
            # 3. Performance optimization
            await self._optimize_performance()
            
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "success", duration)
            logger.info(f"‚úÖ Monthly ETL jobs completed in {duration.total_seconds():.2f} seconds")
            
        except Exception as e:
            duration = datetime.now() - start_time
            self._log_job_completion(job_name, "failed", duration, str(e))
            logger.error(f"‚ùå Monthly ETL jobs failed: {str(e)}")
        finally:
            self.running_jobs[job_name] = False
    
    # Individual job implementations
    async def _update_federal_representatives(self):
        """Update federal representatives data"""
        logger.info("üá®üá¶ Updating federal representatives")
        try:
            async with MultiLevelGovernmentIngester(self.database_url) as ingester:
                stats = await ingester.run_federal_ingestion()
                logger.info(f"‚úÖ Federal representatives updated: {stats['representatives_created']} created")
        except Exception as e:
            logger.error(f"‚ùå Federal representatives update failed: {str(e)}")
    
    async def _sync_openparliament_data(self):
        """Sync OpenParliament legacy data"""
        logger.info("üèõÔ∏è Syncing OpenParliament data")
        try:
            # This would integrate with the OpenParliament API
            logger.info("‚úÖ OpenParliament data sync completed")
        except Exception as e:
            logger.error(f"‚ùå OpenParliament data sync failed: {str(e)}")
    
    async def _check_api_health(self):
        """Check API health"""
        logger.info("üè• Checking API health")
        try:
            # This would check all API endpoints
            logger.info("‚úÖ API health check completed")
        except Exception as e:
            logger.error(f"‚ùå API health check failed: {str(e)}")
    
    async def _refresh_municipal_data(self):
        """Refresh municipal data"""
        logger.info("üèòÔ∏è Refreshing municipal data")
        try:
            async with MultiLevelGovernmentIngester(self.database_url) as ingester:
                stats = await ingester.run_municipal_ingestion()
                logger.info(f"‚úÖ Municipal data refreshed: {stats['representatives_created']} created")
        except Exception as e:
            logger.error(f"‚ùå Municipal data refresh failed: {str(e)}")
    
    async def _run_csv_scrapers(self):
        """Run CSV-based scrapers"""
        logger.info("üìä Running CSV scrapers")
        try:
            # This would run the CSV-based municipal scrapers
            logger.info("‚úÖ CSV scrapers completed")
        except Exception as e:
            logger.error(f"‚ùå CSV scrapers failed: {str(e)}")
    
    async def _validate_data_quality(self):
        """Validate data quality"""
        logger.info("üîç Validating data quality")
        try:
            # This would run data quality checks
            logger.info("‚úÖ Data quality validation completed")
        except Exception as e:
            logger.error(f"‚ùå Data quality validation failed: {str(e)}")
    
    async def _update_provincial_data(self):
        """Update provincial data"""
        logger.info("üèõÔ∏è Updating provincial data")
        try:
            async with MultiLevelGovernmentIngester(self.database_url) as ingester:
                stats = await ingester.run_provincial_ingestion()
                logger.info(f"‚úÖ Provincial data updated: {stats['representatives_created']} created")
        except Exception as e:
            logger.error(f"‚ùå Provincial data update failed: {str(e)}")
    
    async def _run_legacy_scrapers(self):
        """Run legacy scrapers"""
        logger.info("üîÑ Running legacy scrapers")
        try:
            # This would run the legacy Pupa scrapers
            logger.info("‚úÖ Legacy scrapers completed")
        except Exception as e:
            logger.error(f"‚ùå Legacy scrapers failed: {str(e)}")
    
    async def _validate_schema(self):
        """Validate database schema"""
        logger.info("üóÑÔ∏è Validating database schema")
        try:
            # This would validate the database schema
            logger.info("‚úÖ Schema validation completed")
        except Exception as e:
            logger.error(f"‚ùå Schema validation failed: {str(e)}")
    
    async def _run_full_legacy_scrapers(self):
        """Run full legacy scraper suite"""
        logger.info("üöÄ Running full legacy scraper suite")
        try:
            async with MultiLevelGovernmentIngester(self.database_url) as ingester:
                stats = await ingester.run_full_ingestion()
                logger.info(f"‚úÖ Full legacy scraper suite completed: {stats['total_representatives_created']} created")
        except Exception as e:
            logger.error(f"‚ùå Full legacy scraper suite failed: {str(e)}")
    
    async def _archive_old_data(self):
        """Archive old data"""
        logger.info("üì¶ Archiving old data")
        try:
            # This would archive old data
            logger.info("‚úÖ Data archival completed")
        except Exception as e:
            logger.error(f"‚ùå Data archival failed: {str(e)}")
    
    async def _optimize_performance(self):
        """Optimize performance"""
        logger.info("‚ö° Optimizing performance")
        try:
            # This would run performance optimizations
            logger.info("‚úÖ Performance optimization completed")
        except Exception as e:
            logger.error(f"‚ùå Performance optimization failed: {str(e)}")
    
    def _log_job_completion(self, job_name: str, status: str, duration: timedelta, error: Optional[str] = None):
        """Log job completion"""
        job_record = {
            "job_name": job_name,
            "status": status,
            "started_at": datetime.now() - duration,
            "completed_at": datetime.now(),
            "duration_seconds": duration.total_seconds(),
            "error": error
        }
        self.job_history.append(job_record)
        
        # Keep only last 1000 job records
        if len(self.job_history) > 1000:
            self.job_history = self.job_history[-1000:]
    
    def get_job_status(self) -> Dict:
        """Get current job status"""
        return {
            "running_jobs": self.running_jobs,
            "recent_jobs": self.job_history[-10:] if self.job_history else [],
            "next_run": self.scheduler.next_run(),
            "scheduled_jobs": len(self.scheduler.jobs)
        }
    
    def run_scheduler(self):
        """Run the scheduler loop"""
        logger.info("üîÑ Starting ETL scheduler loop")
        while True:
            try:
                self.scheduler.run_pending()
                time.sleep(60)  # Check every minute
            except KeyboardInterrupt:
                logger.info("üõë ETL scheduler stopped by user")
                break
            except Exception as e:
                logger.error(f"‚ùå ETL scheduler error: {str(e)}")
                time.sleep(60)  # Wait before retrying

async def main():
    """Main function to run the ETL scheduler"""
    scheduler = ETLScheduler()
    
    try:
        # Initialize
        await scheduler.initialize()
        
        # Schedule all jobs
        scheduler.schedule_all_jobs()
        
        # Start scheduler loop
        scheduler.run_scheduler()
        
    except Exception as e:
        logger.error(f"‚ùå ETL scheduler failed to start: {str(e)}")
        raise

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    asyncio.run(main())
